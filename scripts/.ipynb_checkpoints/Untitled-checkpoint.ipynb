{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa66f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "cap = cv2.VideoCapture('../videos/peopleWalking.mp4')\n",
    "while cap.isOpened() == False:\n",
    "    print(\"Error opening video stream or file\")\n",
    "while(cap.isOpened()):\n",
    "    ret,frame = cap.read()\n",
    "    if ret == True:\n",
    "        cv2.imshow(\"People Walking\",frame)\n",
    "        if cv2.waitKey(25) == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09de3276",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_3584/62637764.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\figui\\AppData\\Local\\Temp/ipykernel_3584/62637764.py\"\u001b[1;36m, line \u001b[1;32m19\u001b[0m\n\u001b[1;33m    d['box'str(person_id)+'x']=x\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "contn_frame_with_detection=0\n",
    "skipped_frames=0\n",
    "while cap.isOpened():\n",
    "    frame_violator=0\n",
    "    l=[]\n",
    "    d={}\n",
    "    person_id=0\n",
    "    ret,frame = cap.read()\n",
    "    if not ret:\n",
    "      break\n",
    "    frame = cv2.resize(frame, None,fx=0.95, fy=0.95, interpolation = cv2.INTER_LANCZOS4)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    persons = person_clf.detectMultiScale(gray, 1.02, 5)\n",
    "    \n",
    "    \n",
    "    for (x,y,w,h) in persons:\n",
    "        \n",
    "        d['box'str(person_id)+'x']=x\n",
    "        d['box'+str(person_id)+'y']=y\n",
    "        d['box'+str(person_id)+'x+w']=x+w\n",
    "        d['box'+str(person_id)+'y+h']=y+h\n",
    "        l.append(center(int(person_id)))\n",
    "        if person_id>0:\n",
    "            s=0\n",
    "            feed_detected=0\n",
    "            for mid in l[:-1]:\n",
    "                dist = math.sqrt((mid[0] — center(int(person_id))    [0])**2 + (mid[1] — center(int(person_id))[1])**2)\n",
    "                if dist<=40:\n",
    "                cv2.rectangle(frame, (d[‘box’+str(s)+’x’], d[‘box’+str(s)+’y’]), (d[‘box’+str(s)+’x+w’], d[‘box’+str(s)+’y+h’]), (0, 0,255), 2)\n",
    "                cv2.line(frame,(int(mid[0]),int(mid[1])),(center(int(person_id))[0],center(int(person_id))[1]),(0,0,255),(2))\n",
    "                feed_detected+=1\n",
    "                frame_violator+=1\n",
    "                s+=1\n",
    "            if feed_detected>0:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                person_id+=1\n",
    "            if frame_violator>0 and skipped_frames<4:\n",
    "                contn_frame_with_detection+=1\n",
    "                skipped_frames=0\n",
    "            else:\n",
    "                skipped_frames+=1\n",
    "                if skipped_frames>=4:\n",
    "                    ds\n",
    "                    contn_frame_with_detection=0\n",
    "                    skipped_frames=0\n",
    "                    i+=1\n",
    "                if contn_frame_with_detection>25:\n",
    "                    contn_frame_with_detection=0\n",
    "                    skipped_frames=0\n",
    "                    name=”pic”+str(i)+”.jpg”\n",
    "                    cv2.imwrite(‘/GD/My Drive/dataset/’+name,frame)\n",
    "                    mailer(name)\n",
    "                if writer is None:\n",
    "                    vid_write= cv2.VideoWriter_fourcc(*”XVID”)\n",
    "                    writer = cv2.VideoWriter(OUTPUT, vid_write, 35,(frame.shape[1],    frame.shape[0]), True)\n",
    "                    writer.write(frame)\n",
    "\n",
    "\n",
    "writer.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74bca80a",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:207: error: (-212:Parsing error) Failed to parse NetParameter file: data/model/yolov3.cfg in function 'cv::dnn::dnn4_v20211220::readNetFromDarknet'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3584/3760051715.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0myolo_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data/model/yolov3.cfg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mcoco_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data/model/coco.names\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myolo_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myolo_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetPreferableBackend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNN_BACKEND_CUDA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:207: error: (-212:Parsing error) Failed to parse NetParameter file: data/model/yolov3.cfg in function 'cv::dnn::dnn4_v20211220::readNetFromDarknet'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "from itertools import combinations\n",
    "import math\n",
    " \n",
    "# Euclidean Distance between two points\n",
    "def calculateDistance(x1,y1,x2,y2):\n",
    "    dist = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "    return dist\n",
    " \n",
    "# Load Yolo\n",
    "yolo_weight = \"data/model/yolov3.weights\"\n",
    "yolo_config = \"data/model/yolov3.cfg\"\n",
    "coco_labels = \"data/model/coco.names\"\n",
    "net = cv2.dnn.readNet(yolo_weight, yolo_config)\n",
    " \n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    " \n",
    "classes = []\n",
    "with open(coco_labels, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    " \n",
    "# Defining desired shape\n",
    "fWidth = 256\n",
    "fHeight = 256\n",
    " \n",
    "# Below function will read video frames\n",
    "cap = cv2.VideoCapture('../videos/peopleWalking.mp4')\n",
    " \n",
    "while True:\n",
    "    read_ok, img = cap.read()\n",
    " \n",
    "    height, width, channels = img.shape\n",
    " \n",
    "    # Detecting objects\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    " \n",
    "    # Showing information on the screen\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    center_points = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    " \n",
    "            # class_id = 0 means we will only detect persons from video\n",
    "            if confidence > 0.5 and class_id == 0:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h, center_x, center_y])\n",
    "                center_points.append([center_x, center_y])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    " \n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    " \n",
    "    # Create combination list of center points between each detected person bounding box\n",
    "    combination_points = list(combinations(center_points, 2))\n",
    " \n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h, box_center_x, box_center_y = boxes[i]\n",
    " \n",
    "            color = colors[i]\n",
    "            cv2.circle(img, (center_x, center_y), 3, (0, 0, 255), cv2.FILLED)\n",
    " \n",
    "            for points in combination_points:\n",
    "                # Find Distance between two person (pixel distance / apart)\n",
    "                center_x, center_y = points[0]\n",
    "                prev_center_x, prev_center_y = points[1]\n",
    "                euclidean_distance = calculateDistance(center_x, center_y, prev_center_x, prev_center_y)\n",
    " \n",
    "                # Width of three tiles = 217 pixel , which is 9 feet\n",
    "                width_of_3_tiles = 217\n",
    " \n",
    "                # Mark person bounding box as red is distance is less than 9 feet\n",
    "                if euclidean_distance < width_of_3_tiles and euclidean_distance > 150:\n",
    "                    if box_center_x == center_x or box_center_y == center_y:\n",
    "                        # Draw rectangle for each person\n",
    "                        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "                        # Draw line between each person\n",
    "                        cv2.line(img, (center_x, center_y), (prev_center_x, prev_center_y), (0, 255, 0), thickness=2)\n",
    " \n",
    " \n",
    "    cv2.imshow(\"Image\", img)\n",
    "    # Close video window by pressing 'x'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('x'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a5b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75496c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b45f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee7dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8eedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6cd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc607d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture('../videos/peopleWalking.mp4.mp4')\n",
    "fullBody = cv2.CascadeClassifier('../haar/haarcascade_fullbody.xml')\n",
    "status , photo = cap.read()\n",
    "while True:\n",
    "    status , photo = cap.read() \n",
    "    face_cor = fullBody.detectMultiScale(photo,1.8,1)\n",
    "    l = len(face_cor)\n",
    "    photo = cv2.putText(photo, str(len(face_cor))+\" Face\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX,  \n",
    "                   1, (255, 0, 0) , 2, cv2.LINE_AA) \n",
    "    stack_x = [] \n",
    "    stack_y = []\n",
    "    stack_x_print = [] \n",
    "    stack_y_print = []\n",
    "    global D\n",
    "    \n",
    "    if len(face_cor) == 0:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        for i in range(0,len(face_cor)):\n",
    "            x1 = face_cor[i][0]\n",
    "            y1 = face_cor[i][1]\n",
    "            x2 = face_cor[i][0] + face_cor[i][2]\n",
    "            y2 = face_cor[i][1] + face_cor[i][3]\n",
    "            \n",
    "             \n",
    "            mid_x = int((x1+x2)/2)\n",
    "            mid_y = int((y1+y2)/2)\n",
    "            stack_x.append(mid_x)\n",
    "            stack_y.append(mid_y)\n",
    "            stack_x_print.append(mid_x)\n",
    "            stack_y_print.append(mid_y)\n",
    "            \n",
    "            photo = cv2.circle(photo, (mid_x, mid_y), 3 , [255,0,0] , -1) \n",
    "            photo = cv2.rectangle(photo , (x1, y1) , (x2,y2) , [0,255,0] , 2)\n",
    "        \n",
    "        if len(face_cor) == 2:\n",
    "            D = int(dist.euclidean((stack_x.pop(), stack_y.pop()), (stack_x.pop(), stack_y.pop())))\n",
    "            photo = cv2.line(photo, (stack_x_print.pop(), stack_y_print.pop()), (stack_x_print.pop(), stack_y_print.pop()), [0,0,255], 2)\n",
    "        else:\n",
    "            D = 0\n",
    "        \n",
    "        if D<250 and D!=0:\n",
    "            photo = cv2.putText(photo, \"!!MOVE AWAY!!\", (120, 100), cv2.FONT_HERSHEY_SIMPLEX,2, [0,0,255] , 4)\n",
    "            \n",
    "        photo = cv2.putText(photo, str(D/10) + \" cm\", (300, 50), cv2.FONT_HERSHEY_SIMPLEX,  \n",
    "                   1, (255, 0, 0) , 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('hi' , photo)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafabe24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32523b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586bab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f241766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " donner le seuil à respecter : 1233\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "cap = cv2.VideoCapture('../videos/peopleWalking.mp4')\n",
    "fullBody = cv2.CascadeClassifier('../haar/haarcascade_fullbody.xml')\n",
    "seuil = float(input(' donner le seuil à respecter : '))\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    if ret == True:\n",
    "        bodies = fullBody.detectMultiScale(frame,1.8,1)\n",
    "        for(x,y,w,h) in bodies:\n",
    "            dis=0\n",
    "            f=False\n",
    "            for(x1,y1,w1,h1) in bodies:\n",
    "                if x!=x1 and y!=y1:\n",
    "                    dis = math.sqrt(((x+w//2)-(x1+w1//2))**2+((y+h//2)-(y1+h//2))**2)\n",
    "                    if dis<seuil:\n",
    "                        f = True\n",
    "                if f :\n",
    "                    cv2.rectangle(frame,(x,y),(x+w,y+w),(0,255,0),2)\n",
    "                else :\n",
    "                    cv2.rectangle(frame,(x,y),(x+w,y+w),(0,0,255),2)\n",
    "        cv2.imshow(\"People Walking\",frame)\n",
    "        if cv2.waitKey(25) == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b38d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faces_dist(classifier, ref_width, ref_pix):\n",
    "   ratio_px_cm = ref_width / ref_pix\n",
    "   cap = cv2.VideoCapture(0)\n",
    "   while True:       \n",
    "       _, img = cap.read()\n",
    "       gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "       # detect all the faces on the image\n",
    "       faces = classifier.detectMultiScale( gray, 1.1, 4)\n",
    "       annotate_faces( img, faces, ratio_px_cm)\n",
    " \n",
    "       k = cv2.waitKey(30) & 0xff\n",
    "       # use ESC to terminate\n",
    "       if k==27:\n",
    "           break\n",
    "   # release the camera\n",
    "   cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc18e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_faces( img, faces, ratio_px_cm ):\n",
    "   points = []\n",
    "   for (x, y, w, h) in faces:       \n",
    "       center = (x+(int(w/2)), y+(int(h/2)))\n",
    "       cv2.circle( img, center, 2, (0,255,0),2)\n",
    "       for p in points:\n",
    "           ed = euclidean_dist( p, center ) * ratio_px_cm\n",
    "           color = (0,255,0)\n",
    "           if ed < MIN_DIST:\n",
    "               color = (0,0,255)\n",
    "           # draw a rectangle over each detected face\n",
    "           cv2.rectangle( img, (x, y), (x+w, y+h), color, 2)           \n",
    "           # put the distance as text over the face's rectangle\n",
    "           cv2.putText( img, \"%scm\" % (ed),\n",
    "               (x, h -10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               0.5, color, 2)\n",
    "           # draw a line between the faces detected\n",
    "           cv2.line( img, center, p, color, 5)\n",
    "       points.append( center )\n",
    " \n",
    "   cv2.imshow('img', img )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference_pixels(image_path, ref_distance, ref_width):\n",
    "   # open reference image\n",
    "   image = cv2.imread( image_path )\n",
    "   edged = get_edged( image )\n",
    "   # detect all contours over the gray image\n",
    "   allContours = cv2.findContours( edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE )\n",
    "   allContours = imutils.grab_contours( allContours )\n",
    "   markerContour = get_marker( allContours )\n",
    "   # use the marker width to calculate the focal length of the camera\n",
    "   pixels = (cv2.minAreaRect( markerContour ))[1][0]\n",
    "   return pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "#classifier = cv2.CascadeClassifier('lbpcascade_frontalface_improved.xml')\n",
    " \n",
    "IMG_SRC = sys.argv[1]\n",
    "REF_DISTANCE = float(sys.argv[2])\n",
    "REF_WIDTH = float(sys.argv[3])\n",
    "FL = largest_marker_focal_len( IMG_SRC, REF_DISTANCE, REF_WIDTH )\n",
    "faces_dist(classifier, REF_WIDTH, REF_WIDTH, FL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1c219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
